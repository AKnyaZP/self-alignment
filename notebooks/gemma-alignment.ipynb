{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9488177,"sourceType":"datasetVersion","datasetId":5772377},{"sourceId":9761435,"sourceType":"datasetVersion","datasetId":5977654}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IMPORT**","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers\n!pip install -U accelerate bitsandbytes\n!pip install -U peft\n!pip install --upgrade peft\n!pip install bitsandbytes\n!pip install deepspeed\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T22:21:01.922171Z","iopub.execute_input":"2024-10-31T22:21:01.922545Z","iopub.status.idle":"2024-10-31T22:22:49.034130Z","shell.execute_reply.started":"2024-10-31T22:21:01.922509Z","shell.execute_reply":"2024-10-31T22:22:49.033083Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\nCollecting transformers\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed tokenizers-0.20.1 transformers-4.46.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.0)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-1.0.1 bitsandbytes-0.44.1\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.0.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.0.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nCollecting deepspeed\n  Downloading deepspeed-0.15.3.tar.gz (1.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.0.8)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.9.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.66.4)\nRequirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from deepspeed) (11.495.46)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed) (3.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\nDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.15.3-py3-none-any.whl size=1526209 sha256=07a19cd15e3a7a43103b4197084535a7cb80385fca62b36b43ab84085de7822f\n  Stored in directory: /root/.cache/pip/wheels/b3/c2/9f/37a2c813b8d64d7908793319cfdfa4f852754e177f20f0b858\nSuccessfully built deepspeed\nInstalling collected packages: hjson, deepspeed\nSuccessfully installed deepspeed-0.15.3 hjson-3.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom transformers import BitsAndBytesConfig\nimport pandas as pd\nimport numpy as np\n\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom datasets import Dataset\n\nfrom tqdm import tqdm\nimport ast\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient() # Ğ·Ğ´ĞµÑÑŒ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑÑ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¾Ñ‚ HF;)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:22:49.036229Z","iopub.execute_input":"2024-10-31T22:22:49.036598Z","iopub.status.idle":"2024-10-31T22:23:06.829139Z","shell.execute_reply.started":"2024-10-31T22:22:49.036562Z","shell.execute_reply":"2024-10-31T22:23:06.828134Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print('Automatic Mixed Precision (AMP) is supported.')\nelse:\n    print('Automatic Mixed Precision (AMP) is not supported.')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:06.830451Z","iopub.execute_input":"2024-10-31T22:23:06.831053Z","iopub.status.idle":"2024-10-31T22:23:06.835997Z","shell.execute_reply.started":"2024-10-31T22:23:06.831019Z","shell.execute_reply":"2024-10-31T22:23:06.835125Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Automatic Mixed Precision (AMP) is supported.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:06.837737Z","iopub.execute_input":"2024-10-31T22:23:06.838022Z","iopub.status.idle":"2024-10-31T22:23:07.898988Z","shell.execute_reply.started":"2024-10-31T22:23:06.837991Z","shell.execute_reply":"2024-10-31T22:23:07.897743Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Thu Oct 31 22:23:07 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             26W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:07.900630Z","iopub.execute_input":"2024-10-31T22:23:07.901075Z","iopub.status.idle":"2024-10-31T22:23:07.906198Z","shell.execute_reply.started":"2024-10-31T22:23:07.901029Z","shell.execute_reply":"2024-10-31T22:23:07.905323Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **MODEL**","metadata":{}},{"cell_type":"code","source":"access_token = user_secrets.get_secret(\"hf_token\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token=access_token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    token=access_token)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:07.907637Z","iopub.execute_input":"2024-10-31T22:23:07.908242Z","iopub.status.idle":"2024-10-31T22:25:19.819760Z","shell.execute_reply.started":"2024-10-31T22:23:07.908208Z","shell.execute_reply":"2024-10-31T22:25:19.818774Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0752a5809cba4a45aa861f3d950f82b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37b0a7fbe1b47cb96ffbec294dce155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea6fefb919f483da4eac2b5856ed3f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f4b44e04944879bec5e0239313eed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7480bf54ac42499db1e873b254062029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4285957cb26481e9e51926e5f732c5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22582bfb6b7e4454a4d9e64034675014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f00da274f70e4e5ab837857a7ed1a505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6b978ec0634dca875ed57833329a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c97d7cbf154dce8c4dbcec8dbbdb74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e39c7b6508f4a7aba8ec1363c88b2aa"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"%time \n\nmessages = [\n    {\"role\": \"user\", \"content\": \"ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼. Ğ—Ğ½Ğ°ĞµÑˆÑŒ Ğ»Ğ¸ Ñ‚Ñ‹ Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ quality Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ OpenAssistant/oasst1\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n\noutputs = model.generate(**input_ids, max_new_tokens=1024)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:31:36.615895Z","iopub.status.idle":"2024-10-31T21:31:36.616280Z","shell.execute_reply.started":"2024-10-31T21:31:36.616075Z","shell.execute_reply":"2024-10-31T21:31:36.616094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATA**","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/data4self-alignment/data4alignment_train_mean_label.csv')\ntest_data = pd.read_csv('/kaggle/input/data4self-alignment/data4alignment_test_mean_label.csv')\n\ntrain_data['labels'] = train_data['estimate']\ntest_data['labels'] = test_data['estimate']\n\ntrain_data = train_data.drop(['estimate'], axis=1)\ntest_data = test_data.drop(['estimate'], axis=1)\n\n\nprint(train_data.head())\nprint()\nprint(test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:25:19.821063Z","iopub.execute_input":"2024-10-31T22:25:19.821449Z","iopub.status.idle":"2024-10-31T22:25:19.878773Z","shell.execute_reply.started":"2024-10-31T22:25:19.821411Z","shell.execute_reply":"2024-10-31T22:25:19.877767Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                         instruction  \\\n0                    ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¸Ğ³Ñ€Ğµ Hytale   \n1              Ğ£ Ğ¼ĞµĞ½Ñ ÑƒĞ³Ğ½Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğµ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ?   \n2  ĞšĞ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ‘ÑƒĞ»Ğ³Ğ°ĞºĞ¾Ğ²Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ˜ĞµÑ€ÑƒÑĞ°Ğ»...   \n3  ĞŸĞ»ÑÑÑ‹ Ğ¸ Ğ¼Ğ¸Ğ½ÑƒÑÑ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ C++ Ğ¸ R...   \n4                Ğ§Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ ĞµÑĞ»Ğ¸ Ğ¸Ğ·Ğ´ĞµĞ²Ğ°ÑÑ‚ÑÑ Ğ² ÑˆĞºĞ¾Ğ»Ğµ?   \n\n                                              answer  labels  \n0   Hytale - ÑÑ‚Ğ¾ Ğ¸Ğ³Ñ€Ğ° Ğ² Ğ¶Ğ°Ğ½Ñ€Ğµ sandbox, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½...       5  \n1   \\n1. ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ†Ğ¸Ñ.\\n2. Ğ¡Ğ¾Ğ¾Ğ±Ñ‰Ğ¸Ñ‚ÑŒ Ğ¾ ĞºÑ€Ğ°Ğ¶...       5  \n2  \\n\\n* \"ĞœĞ°ÑÑ‚ĞµÑ€ Ğ¸ ĞœĞ°Ñ€Ğ³Ğ°Ñ€Ğ¸Ñ‚Ğ°\"\\n* \"Ğ¡Ğ¾Ğ±Ğ°Ñ‡ÑŒĞµ ÑĞµÑ€Ğ´Ñ†Ğµ\"...       5  \n3  \\n\\n**ĞŸĞ»ÑÑÑ‹ C++:**\\n* Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ñ...       4  \n4  \\n\\n1. **ĞĞµ Ğ±Ğ¾Ğ¹ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒÑÑ Ğ·Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ.**  ĞŸĞ¾...       5  \n\n                                         instruction  \\\n0  ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ swift, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ñƒ...   \n1                        Ğ§ĞµĞ¼Ñƒ Ñ€Ğ°Ğ²ĞµĞ½ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ»ÑŒ?   \n2                       Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ ÑĞ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ?   \n3  ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ ĞºĞ°Ğº ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ€Ğ°ĞºĞµÑ‚Ñƒ Ğ² Ğ³Ğ°Ñ€...   \n4  Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸, ĞµÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ¼ - Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾...   \n\n                                              answer  labels  \n0  \\n\\n```swift\\nfunc sortAndPrintArray(array: [I...       5  \n1                                              0\\n\\n       5  \n2   Ğ¡Ğ²ĞµÑ€Ñ…Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ - ÑÑ‚Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ°...       5  \n3  \\n\\n1. ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚Ğµ Ñ†ĞµĞ»ÑŒ Ğ¸ Ñ‚Ğ¸Ğ¿ Ñ€Ğ°ĞºĞµÑ‚Ñ‹.\\n2. Ğ¡Ğ¾Ğ±ĞµÑ€...       4  \n4  \\n\\n* \"Ğ›ÑƒÑ‡ÑˆĞµ Ğ±Ñ‹Ñ‚ÑŒ Ñ…Ğ¸Ñ‰Ğ½Ğ¸ĞºĞ¾Ğ¼, Ñ‡ĞµĞ¼ Ñ…Ğ¸Ñ‰Ğ½Ğ¸ĞºĞ¾Ğ¼-Ğ¿Ğ¾Ğ±ĞµĞ´...       4  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#### ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°Ñ„Ñ€ĞµĞ¹Ğ¼Ğ° Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ `input_ids` Ğ¸ `attention_mask` Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸","metadata":{}},{"cell_type":"code","source":"def preprocess_data(examples):\n    instructions = [str(inst) if inst is not None else \"\" for inst in examples[\"instruction\"]]\n    answers = [str(ans) if ans is not None else \"\" for ans in examples[\"answer\"]]\n\n    instruction_tokenized = tokenizer(\n        instructions,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\" \n    )\n\n    answer_tokenized = tokenizer(\n        answers,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\"\n    )\n\n    return {\n        \"input_ids\": instruction_tokenized[\"input_ids\"],\n        \"attention_mask\": instruction_tokenized[\"attention_mask\"],\n        \"labels\": answer_tokenized[\"input_ids\"]\n    }\n\ntrain_dataset = Dataset.from_pandas(train_data)\ntrain_dataset = train_dataset.map(preprocess_data, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:25:20.095875Z","iopub.execute_input":"2024-10-31T22:25:20.096161Z","iopub.status.idle":"2024-10-31T22:25:20.895991Z","shell.execute_reply.started":"2024-10-31T22:25:20.096127Z","shell.execute_reply":"2024-10-31T22:25:20.895127Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/617 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d944a7b5814b849235f1b2c232335c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"test_dataset = Dataset.from_pandas(test_data)\ntest_dataset = test_dataset.map(preprocess_data, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:25:20.897159Z","iopub.execute_input":"2024-10-31T22:25:20.897491Z","iopub.status.idle":"2024-10-31T22:25:21.446166Z","shell.execute_reply.started":"2024-10-31T22:25:20.897456Z","shell.execute_reply":"2024-10-31T22:25:21.445156Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a7ed5c3e464d07a568dea53e8cc375"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:25:21.449411Z","iopub.execute_input":"2024-10-31T22:25:21.449723Z","iopub.status.idle":"2024-10-31T22:25:21.457365Z","shell.execute_reply.started":"2024-10-31T22:25:21.449690Z","shell.execute_reply":"2024-10-31T22:25:21.456490Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'answer', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 617\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# **Alignment**","metadata":{}},{"cell_type":"markdown","source":"## DPO (Direct Preference Optimization)","metadata":{}},{"cell_type":"code","source":"# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ PEFT\nconfig = LoraConfig(\n    r=16,   \n    lora_alpha=32, \n    target_modules=[\"q_proj\", \"v_proj\"], # ĞœĞ¾Ğ´ÑƒĞ»Ğ¸, Ğº ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ LoRA\n    lora_dropout=0.1,\n    bias=\"none\",  # ĞĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ²ĞµÑĞ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹\n    task_type=\"CAUSAL_LM\"  # Ğ¢Ğ¸Ğ¿ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ\n)\nmodel = get_peft_model(model, config)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:45:29.858691Z","iopub.execute_input":"2024-10-31T22:45:29.859147Z","iopub.status.idle":"2024-10-31T22:45:30.033342Z","shell.execute_reply.started":"2024-10-31T22:45:29.859106Z","shell.execute_reply":"2024-10-31T22:45:30.032325Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n              (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n              (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    report_to=\"wandb\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T23:01:13.047904Z","iopub.execute_input":"2024-10-31T23:01:13.048575Z","iopub.status.idle":"2024-10-31T23:01:13.078685Z","shell.execute_reply.started":"2024-10-31T23:01:13.048532Z","shell.execute_reply":"2024-10-31T23:01:13.077769Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        device = model.device\n\n        # Move inputs to device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Model forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        if logits.shape[1] != labels.shape[1]:\n            labels = labels[:, :logits.shape[1]] \n\n        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n\n        return (loss, outputs) if return_outputs else loss\n\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:45:34.169899Z","iopub.execute_input":"2024-10-31T22:45:34.170743Z","iopub.status.idle":"2024-10-31T22:45:37.276882Z","shell.execute_reply.started":"2024-10-31T22:45:34.170688Z","shell.execute_reply":"2024-10-31T22:45:37.275871Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[2024-10-31 22:45:34,221] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"#### **Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸**","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\nwandb.login() \nwandb.init(project=\"alignment-gemma\")  \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T23:09:16.160481Z","iopub.execute_input":"2024-10-31T23:09:16.161276Z","iopub.status.idle":"2024-10-31T23:18:39.210379Z","shell.execute_reply.started":"2024-10-31T23:09:16.161236Z","shell.execute_reply":"2024-10-31T23:18:39.209354Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:ym7mrltr) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"456d1e2b529b47809ec9ec3fb3e11ed5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–</td></tr><tr><td>eval/runtime</td><td>â–ˆâ–â–</td></tr><tr><td>eval/samples_per_second</td><td>â–â–ˆâ–ˆ</td></tr><tr><td>eval/steps_per_second</td><td>â–â–ˆâ–ˆ</td></tr><tr><td>train/epoch</td><td>â–â–…â–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–…â–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>9.01339</td></tr><tr><td>eval/runtime</td><td>4.8871</td></tr><tr><td>eval/samples_per_second</td><td>7.162</td></tr><tr><td>eval/steps_per_second</td><td>1.842</td></tr><tr><td>total_flos</td><td>2882531584180224.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>465</td></tr><tr><td>train_loss</td><td>9.9002</td></tr><tr><td>train_runtime</td><td>570.7574</td></tr><tr><td>train_samples_per_second</td><td>3.243</td></tr><tr><td>train_steps_per_second</td><td>0.815</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/ai-sigma/huggingface/runs/ym7mrltr' target=\"_blank\">https://wandb.ai/ai-sigma/huggingface/runs/ym7mrltr</a><br/> View project at: <a href='https://wandb.ai/ai-sigma/huggingface' target=\"_blank\">https://wandb.ai/ai-sigma/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241031_224549-ym7mrltr/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:ym7mrltr). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241031_230916-yrns0c1p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ai-sigma/alignment-gemma/runs/yrns0c1p' target=\"_blank\">moonlit-goblin-1</a></strong> to <a href='https://wandb.ai/ai-sigma/alignment-gemma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ai-sigma/alignment-gemma' target=\"_blank\">https://wandb.ai/ai-sigma/alignment-gemma</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ai-sigma/alignment-gemma/runs/yrns0c1p' target=\"_blank\">https://wandb.ai/ai-sigma/alignment-gemma/runs/yrns0c1p</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [465/465 09:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>8.581250</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>8.357142</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>8.212500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6724104a-5434abf523cdb7311be6f04c;be34c400-cc0a-4809-a76d-745adf8c6abe)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-2-2b-it.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in google/gemma-2-2b-it - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=465, training_loss=7.598185483870968, metrics={'train_runtime': 558.2755, 'train_samples_per_second': 3.316, 'train_steps_per_second': 0.833, 'total_flos': 2882531584180224.0, 'train_loss': 7.598185483870968, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"#### *Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ DPO* ","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_gemma_model_dpo\")\ntokenizer.save_pretrained(\"./fine_tuned__gemma_model_dpo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T23:18:49.501963Z","iopub.execute_input":"2024-10-31T23:18:49.502587Z","iopub.status.idle":"2024-10-31T23:18:50.210445Z","shell.execute_reply.started":"2024-10-31T23:18:49.502539Z","shell.execute_reply":"2024-10-31T23:18:50.209523Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67241059-707c3dc449cc9588567a56cb;519bf457-8814-4b71-baba-d16eecf8580f)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-2-2b-it.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in google/gemma-2-2b-it - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned__gemma_model_dpo/tokenizer_config.json',\n './fine_tuned__gemma_model_dpo/special_tokens_map.json',\n './fine_tuned__gemma_model_dpo/tokenizer.model',\n './fine_tuned__gemma_model_dpo/added_tokens.json',\n './fine_tuned__gemma_model_dpo/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}