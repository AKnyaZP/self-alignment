{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9488177,"sourceType":"datasetVersion","datasetId":5772377},{"sourceId":9761435,"sourceType":"datasetVersion","datasetId":5977654}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IMPORT**","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers\n!pip install -U accelerate bitsandbytes\n!pip install -U peft\n!pip install --upgrade peft\n!pip install bitsandbytes\n!pip install deepspeed\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T22:21:01.922171Z","iopub.execute_input":"2024-10-31T22:21:01.922545Z","iopub.status.idle":"2024-10-31T22:22:49.034130Z","shell.execute_reply.started":"2024-10-31T22:21:01.922509Z","shell.execute_reply":"2024-10-31T22:22:49.033083Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\nCollecting transformers\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed tokenizers-0.20.1 transformers-4.46.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.0)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-1.0.1 bitsandbytes-0.44.1\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.0.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.0.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nCollecting deepspeed\n  Downloading deepspeed-0.15.3.tar.gz (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.0.8)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.9.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.66.4)\nRequirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from deepspeed) (11.495.46)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed) (3.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\nDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.15.3-py3-none-any.whl size=1526209 sha256=07a19cd15e3a7a43103b4197084535a7cb80385fca62b36b43ab84085de7822f\n  Stored in directory: /root/.cache/pip/wheels/b3/c2/9f/37a2c813b8d64d7908793319cfdfa4f852754e177f20f0b858\nSuccessfully built deepspeed\nInstalling collected packages: hjson, deepspeed\nSuccessfully installed deepspeed-0.15.3 hjson-3.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom transformers import BitsAndBytesConfig\nimport pandas as pd\nimport numpy as np\n\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom datasets import Dataset\n\nfrom tqdm import tqdm\nimport ast\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient() # здесь хранится токен от HF;)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:22:49.036229Z","iopub.execute_input":"2024-10-31T22:22:49.036598Z","iopub.status.idle":"2024-10-31T22:23:06.829139Z","shell.execute_reply.started":"2024-10-31T22:22:49.036562Z","shell.execute_reply":"2024-10-31T22:23:06.828134Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print('Automatic Mixed Precision (AMP) is supported.')\nelse:\n    print('Automatic Mixed Precision (AMP) is not supported.')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:06.830451Z","iopub.execute_input":"2024-10-31T22:23:06.831053Z","iopub.status.idle":"2024-10-31T22:23:06.835997Z","shell.execute_reply.started":"2024-10-31T22:23:06.831019Z","shell.execute_reply":"2024-10-31T22:23:06.835125Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Automatic Mixed Precision (AMP) is supported.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:06.837737Z","iopub.execute_input":"2024-10-31T22:23:06.838022Z","iopub.status.idle":"2024-10-31T22:23:07.898988Z","shell.execute_reply.started":"2024-10-31T22:23:06.837991Z","shell.execute_reply":"2024-10-31T22:23:07.897743Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Thu Oct 31 22:23:07 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             26W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:07.900630Z","iopub.execute_input":"2024-10-31T22:23:07.901075Z","iopub.status.idle":"2024-10-31T22:23:07.906198Z","shell.execute_reply.started":"2024-10-31T22:23:07.901029Z","shell.execute_reply":"2024-10-31T22:23:07.905323Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **MODEL**","metadata":{}},{"cell_type":"code","source":"access_token = user_secrets.get_secret(\"hf_token\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token=access_token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    token=access_token)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:23:07.907637Z","iopub.execute_input":"2024-10-31T22:23:07.908242Z","iopub.status.idle":"2024-10-31T22:25:19.819760Z","shell.execute_reply.started":"2024-10-31T22:23:07.908208Z","shell.execute_reply":"2024-10-31T22:25:19.818774Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0752a5809cba4a45aa861f3d950f82b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37b0a7fbe1b47cb96ffbec294dce155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea6fefb919f483da4eac2b5856ed3f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f4b44e04944879bec5e0239313eed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7480bf54ac42499db1e873b254062029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4285957cb26481e9e51926e5f732c5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22582bfb6b7e4454a4d9e64034675014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f00da274f70e4e5ab837857a7ed1a505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6b978ec0634dca875ed57833329a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c97d7cbf154dce8c4dbcec8dbbdb74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e39c7b6508f4a7aba8ec1363c88b2aa"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"%time \n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Отвечай на русском. Знаешь ли ты что такое quality в датасете OpenAssistant/oasst1\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n\noutputs = model.generate(**input_ids, max_new_tokens=1024)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:31:36.615895Z","iopub.status.idle":"2024-10-31T21:31:36.616280Z","shell.execute_reply.started":"2024-10-31T21:31:36.616075Z","shell.execute_reply":"2024-10-31T21:31:36.616094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATA**","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/data4self-alignment/data4alignment_train_mean_label.csv')\ntest_data = pd.read_csv('/kaggle/input/data4self-alignment/data4alignment_test_mean_label.csv')\n\ntrain_data['labels'] = train_data['estimate']\ntest_data['labels'] = test_data['estimate']\n\ntrain_data = train_data.drop(['estimate'], axis=1)\ntest_data = test_data.drop(['estimate'], axis=1)\n\n\nprint(train_data.head())\nprint()\nprint(test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:25:19.821063Z","iopub.execute_input":"2024-10-31T22:25:19.821449Z","iopub.status.idle":"2024-10-31T22:25:19.878773Z","shell.execute_reply.started":"2024-10-31T22:25:19.821411Z","shell.execute_reply":"2024-10-31T22:25:19.877767Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                         instruction  \\\n0                    Напиши информацию о игре Hytale   \n1              У меня угнали машину, что мне делать?   \n2  Какие произведения Булгакова связаны с Иерусал...   \n3  Плюсы и минусы языков программирования C++ и R...   \n4                Что делать если издеваются в школе?   \n\n                                              answer  labels  \n0   Hytale - это игра в жанре sandbox, разработан...       5  \n1   \\n1. Позвонить в полицию.\\n2. Сообщить о краж...       5  \n2  \\n\\n* \"Мастер и Маргарита\"\\n* \"Собачье сердце\"...       5  \n3  \\n\\n**Плюсы C++:**\\n* Высокая производительнос...       4  \n4  \\n\\n1. **Не бойся обратиться за помощью.**  По...       5  \n\n                                         instruction  \\\n0  Напиши функцию на языке swift, которая сортиру...   \n1                        Чему равен абсолютный ноль?   \n2                       Что такое сверхпроводимость?   \n3  Напиши пошаговый план как сделать ракету в гар...   \n4  Смотри, есть такой постироничный мем - глубоко...   \n\n                                              answer  labels  \n0  \\n\\n```swift\\nfunc sortAndPrintArray(array: [I...       5  \n1                                              0\\n\\n       5  \n2   Сверхпроводимость - это свойство некоторых ма...       5  \n3  \\n\\n1. Определите цель и тип ракеты.\\n2. Собер...       4  \n4  \\n\\n* \"Лучше быть хищником, чем хищником-побед...       4  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#### Преобразование датафрейма в датасет с добавлением `input_ids` и `attention_mask` для более качественного дообучения модели","metadata":{}},{"cell_type":"code","source":"def preprocess_data(examples):\n    instructions = [str(inst) if inst is not None else \"\" for inst in examples[\"instruction\"]]\n    answers = [str(ans) if ans is not None else \"\" for ans in examples[\"answer\"]]\n\n    instruction_tokenized = tokenizer(\n        instructions,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\" \n    )\n\n    answer_tokenized = tokenizer(\n        answers,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\"\n    )\n\n    return {\n        \"input_ids\": instruction_tokenized[\"input_ids\"],\n        \"attention_mask\": instruction_tokenized[\"attention_mask\"],\n        \"labels\": answer_tokenized[\"input_ids\"]\n    }\n\ntrain_dataset = Dataset.from_pandas(train_data)\ntrain_dataset = train_dataset.map(preprocess_data, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:25:20.095875Z","iopub.execute_input":"2024-10-31T22:25:20.096161Z","iopub.status.idle":"2024-10-31T22:25:20.895991Z","shell.execute_reply.started":"2024-10-31T22:25:20.096127Z","shell.execute_reply":"2024-10-31T22:25:20.895127Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/617 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d944a7b5814b849235f1b2c232335c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"test_dataset = Dataset.from_pandas(test_data)\ntest_dataset = test_dataset.map(preprocess_data, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:25:20.897159Z","iopub.execute_input":"2024-10-31T22:25:20.897491Z","iopub.status.idle":"2024-10-31T22:25:21.446166Z","shell.execute_reply.started":"2024-10-31T22:25:20.897456Z","shell.execute_reply":"2024-10-31T22:25:21.445156Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a7ed5c3e464d07a568dea53e8cc375"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:25:21.449411Z","iopub.execute_input":"2024-10-31T22:25:21.449723Z","iopub.status.idle":"2024-10-31T22:25:21.457365Z","shell.execute_reply.started":"2024-10-31T22:25:21.449690Z","shell.execute_reply":"2024-10-31T22:25:21.456490Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'answer', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 617\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# **Alignment**","metadata":{}},{"cell_type":"markdown","source":"## DPO (Direct Preference Optimization)","metadata":{}},{"cell_type":"code","source":"# Используем PEFT\nconfig = LoraConfig(\n    r=16,   \n    lora_alpha=32, \n    target_modules=[\"q_proj\", \"v_proj\"], # Модули, к которым применяем LoRA\n    lora_dropout=0.1,\n    bias=\"none\",  # Не обновляем веса смещений\n    task_type=\"CAUSAL_LM\"  # Тип задачи: языковое моделирование\n)\nmodel = get_peft_model(model, config)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:45:29.858691Z","iopub.execute_input":"2024-10-31T22:45:29.859147Z","iopub.status.idle":"2024-10-31T22:45:30.033342Z","shell.execute_reply.started":"2024-10-31T22:45:29.859106Z","shell.execute_reply":"2024-10-31T22:45:30.032325Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n              (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n              (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    report_to=\"wandb\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T23:01:13.047904Z","iopub.execute_input":"2024-10-31T23:01:13.048575Z","iopub.status.idle":"2024-10-31T23:01:13.078685Z","shell.execute_reply.started":"2024-10-31T23:01:13.048532Z","shell.execute_reply":"2024-10-31T23:01:13.077769Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        device = model.device\n\n        # Move inputs to device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Model forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        if logits.shape[1] != labels.shape[1]:\n            labels = labels[:, :logits.shape[1]] \n\n        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n\n        return (loss, outputs) if return_outputs else loss\n\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:45:34.169899Z","iopub.execute_input":"2024-10-31T22:45:34.170743Z","iopub.status.idle":"2024-10-31T22:45:37.276882Z","shell.execute_reply.started":"2024-10-31T22:45:34.170688Z","shell.execute_reply":"2024-10-31T22:45:37.275871Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[2024-10-31 22:45:34,221] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"#### **Запуск обучения модели**","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\nwandb.login() \nwandb.init(project=\"alignment-gemma\")  \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T23:09:16.160481Z","iopub.execute_input":"2024-10-31T23:09:16.161276Z","iopub.status.idle":"2024-10-31T23:18:39.210379Z","shell.execute_reply.started":"2024-10-31T23:09:16.161236Z","shell.execute_reply":"2024-10-31T23:18:39.209354Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:ym7mrltr) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"456d1e2b529b47809ec9ec3fb3e11ed5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▁</td></tr><tr><td>eval/runtime</td><td>█▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁██</td></tr><tr><td>eval/steps_per_second</td><td>▁██</td></tr><tr><td>train/epoch</td><td>▁▅██</td></tr><tr><td>train/global_step</td><td>▁▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>9.01339</td></tr><tr><td>eval/runtime</td><td>4.8871</td></tr><tr><td>eval/samples_per_second</td><td>7.162</td></tr><tr><td>eval/steps_per_second</td><td>1.842</td></tr><tr><td>total_flos</td><td>2882531584180224.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>465</td></tr><tr><td>train_loss</td><td>9.9002</td></tr><tr><td>train_runtime</td><td>570.7574</td></tr><tr><td>train_samples_per_second</td><td>3.243</td></tr><tr><td>train_steps_per_second</td><td>0.815</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/ai-sigma/huggingface/runs/ym7mrltr' target=\"_blank\">https://wandb.ai/ai-sigma/huggingface/runs/ym7mrltr</a><br/> View project at: <a href='https://wandb.ai/ai-sigma/huggingface' target=\"_blank\">https://wandb.ai/ai-sigma/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241031_224549-ym7mrltr/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:ym7mrltr). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241031_230916-yrns0c1p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ai-sigma/alignment-gemma/runs/yrns0c1p' target=\"_blank\">moonlit-goblin-1</a></strong> to <a href='https://wandb.ai/ai-sigma/alignment-gemma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ai-sigma/alignment-gemma' target=\"_blank\">https://wandb.ai/ai-sigma/alignment-gemma</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ai-sigma/alignment-gemma/runs/yrns0c1p' target=\"_blank\">https://wandb.ai/ai-sigma/alignment-gemma/runs/yrns0c1p</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [465/465 09:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>8.581250</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>8.357142</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>8.212500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6724104a-5434abf523cdb7311be6f04c;be34c400-cc0a-4809-a76d-745adf8c6abe)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-2-2b-it.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in google/gemma-2-2b-it - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=465, training_loss=7.598185483870968, metrics={'train_runtime': 558.2755, 'train_samples_per_second': 3.316, 'train_steps_per_second': 0.833, 'total_flos': 2882531584180224.0, 'train_loss': 7.598185483870968, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"#### *Сохраняем модель выравненную при помощи техники DPO* ","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_gemma_model_dpo\")\ntokenizer.save_pretrained(\"./fine_tuned__gemma_model_dpo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T23:18:49.501963Z","iopub.execute_input":"2024-10-31T23:18:49.502587Z","iopub.status.idle":"2024-10-31T23:18:50.210445Z","shell.execute_reply.started":"2024-10-31T23:18:49.502539Z","shell.execute_reply":"2024-10-31T23:18:50.209523Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67241059-707c3dc449cc9588567a56cb;519bf457-8814-4b71-baba-d16eecf8580f)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-2-2b-it.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in google/gemma-2-2b-it - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned__gemma_model_dpo/tokenizer_config.json',\n './fine_tuned__gemma_model_dpo/special_tokens_map.json',\n './fine_tuned__gemma_model_dpo/tokenizer.model',\n './fine_tuned__gemma_model_dpo/added_tokens.json',\n './fine_tuned__gemma_model_dpo/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}